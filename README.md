# Assigment Mechanistic interpretability
**Authors:** Johannes Munker and Serine Benmohra <br/>
**Task Description:** https://jaspock.github.io/tpln2425/assignment-interpretability/ <br/>
**Date:** 2025-01-26

## Description
This study explores the mechanistic interpretability of transformer models when processing short logical reasoning tasks. By examining how these models handle pairs of clean and corrupted logical arguments, we aim to uncover the internal mechanisms that drive their understanding and processing of logical structures. Our investigation focuses on comparing the model's behavior when presented with valid logical sequences against cases where logical coherence is deliberately disrupted. Through this analysis, we seek to identify specific attention patterns and neural circuits that may be responsible for logical reasoning capabilities within the transformer architecture. The methodology and implementation of our research is presented in the subsequent section.

